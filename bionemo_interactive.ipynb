{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioNeMo Container Interactive Access\n",
    "\n",
    "This notebook demonstrates how to interact with the BioNeMo container from a Jupyter notebook running on a SageMaker notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "CONTAINER_NAME = \"bionemo\"\n",
    "\n",
    "def run_in_container(command):\n",
    "    \"\"\"Run a shell command inside the BioNeMo container.\"\"\"\n",
    "    full_cmd = f'docker exec {CONTAINER_NAME} bash -c \"{command}\"'\n",
    "    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    return result.stdout\n",
    "\n",
    "def run_python_in_container(python_code):\n",
    "    \"\"\"Run Python code inside the BioNeMo container by writing to a temp file.\"\"\"\n",
    "    # Write code to a temp file in shared workspace\n",
    "    temp_path = '/home/ec2-user/SageMaker/.temp_script.py'\n",
    "    with open(temp_path, 'w') as f:\n",
    "        f.write(python_code)\n",
    "    \n",
    "    # Run in container\n",
    "    result = run_in_container(\"python /workspace/.temp_script.py\")\n",
    "    return result\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0a0+5228986c39.nv25.06\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "Number of GPUs: 4\n",
      "  GPU 0: NVIDIA A10G\n",
      "  GPU 1: NVIDIA A10G\n",
      "  GPU 2: NVIDIA A10G\n",
      "  GPU 3: NVIDIA A10G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "python_code = \"\"\"\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\"\"\"\n",
    "\n",
    "print(run_python_in_container(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Container is Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container is running: Up 32 minutes\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(\"docker ps --filter name=bionemo --format '{{.Status}}'\", \n",
    "                       shell=True, capture_output=True, text=True)\n",
    "if result.stdout.strip():\n",
    "    print(f\"✅ Container is running: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"❌ Container is not running!\")\n",
    "    print(\"\\nStart it with this command in the terminal:\")\n",
    "    print(\"\"\"\n",
    "docker run -d \\\\\n",
    "    --name bionemo \\\\\n",
    "    --gpus all \\\\\n",
    "    --ipc=host \\\\\n",
    "    --ulimit memlock=-1 \\\\\n",
    "    --ulimit stack=67108864 \\\\\n",
    "    -e TRITON_LIBCUDA_PATH=/usr/lib/x86_64-linux-gnu/libcuda.so.1 \\\\\n",
    "    -v /home/ec2-user/SageMaker:/workspace \\\\\n",
    "    <YOUR_AWS_ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/<IMAGE>:<TAG> \\\\\n",
    "    tail -f /dev/null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  4 02:11:51 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.211.01             Driver Version: 570.211.01     CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   21C    P8             15W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A10G                    On  |   00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   21C    P8             15W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A10G                    On  |   00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   20C    P8             15W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   20C    P8              9W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(run_in_container(\"nvidia-smi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check BioNeMo Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeMo packages installed:\n",
      "bionemo-amplify                  0.0.1\n",
      "bionemo-core                     2.4.5\n",
      "bionemo-esm2                     2.4\n",
      "bionemo-evo2                     2.4\n",
      "bionemo-example_model            0.0.0\n",
      "bionemo-fw                       0.0.0\n",
      "bionemo-geneformer               2.4\n",
      "bionemo-llm                      2.4.5\n",
      "bionemo-moco                     0.0.2.2\n",
      "bionemo-noodles                  0.1.2\n",
      "bionemo-scdl                     0.1.3\n",
      "bionemo-scspeedtest              0.0.1\n",
      "bionemo-size-aware-batching      1.0.0\n",
      "bionemo-testing                  2.4.1\n",
      "bionemo-webdatamodule            1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BioNeMo packages installed:\")\n",
    "print(run_in_container(\"pip list | grep -i bionemo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PyTorch & CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0a0+5228986c39.nv25.06\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "Number of GPUs: 4\n",
      "  GPU 0: NVIDIA A10G\n",
      "  GPU 1: NVIDIA A10G\n",
      "  GPU 2: NVIDIA A10G\n",
      "  GPU 3: NVIDIA A10G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "python_code = \"\"\"\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\"\"\"\n",
    "\n",
    "print(run_python_in_container(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Evo2 Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Evo2 Mamba model sizes:\n",
      "  - hybrid_mamba_8b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "python_code = \"\"\"\n",
    "import logging\n",
    "logging.getLogger('nemo.utils.import_utils').setLevel(logging.ERROR)\n",
    "\n",
    "from bionemo.evo2.models.mamba import MAMBA_MODEL_OPTIONS\n",
    "\n",
    "print('Available Evo2 Mamba model sizes:')\n",
    "for name in MAMBA_MODEL_OPTIONS.keys():\n",
    "    print(f'  - {name}')\n",
    "\"\"\"\n",
    "\n",
    "print(run_python_in_container(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evo2 Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-02-04 02:18:08 nemo_logging:393] Using byte-level tokenization\n",
      "Tokenizing DNA sequences:\n",
      "  ATCGATCGATCG -> [65, 84, 67, 71, 65, 84, 67, 71, 65, 84, 67, 71] -> ATCGATCGATCG\n",
      "  GCTAGCTAGCTA -> [71, 67, 84, 65, 71, 67, 84, 65, 71, 67, 84, 65] -> GCTAGCTAGCTA\n",
      "  AAACCCGGGTTTT -> [65, 65, 65, 67, 67, 67, 71, 71, 71, 84, 84, 84, 84] -> AAACCCGGGTTTT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "python_code = \"\"\"\n",
    "import logging\n",
    "logging.getLogger('nemo.utils.import_utils').setLevel(logging.ERROR)\n",
    "\n",
    "# This is how train_evo2 imports it\n",
    "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer\n",
    "\n",
    "tokenizer = get_nmt_tokenizer('byte-level')\n",
    "\n",
    "sequences = [\n",
    "    'ATCGATCGATCG',\n",
    "    'GCTAGCTAGCTA', \n",
    "    'AAACCCGGGTTTT',\n",
    "]\n",
    "\n",
    "print('Tokenizing DNA sequences:')\n",
    "for seq in sequences:\n",
    "    tokens = tokenizer.text_to_ids(seq)\n",
    "    decoded = tokenizer.ids_to_text(tokens)\n",
    "    print(f'  {seq} -> {tokens} -> {decoded}')\n",
    "\"\"\"\n",
    "print(run_python_in_container(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Class for Easy Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioNeMoContainer helper created!\n",
      "\n",
      "Usage:\n",
      "  bionemo.run(\"nvidia-smi\")\n",
      "  bionemo.python(\"import torch; print(torch.cuda.is_available())\")\n",
      "  bionemo.gpu_memory()\n",
      "  bionemo.list_files(\"/workspace\")\n",
      "  bionemo.script(\"/workspace/my_script.py\")\n"
     ]
    }
   ],
   "source": [
    "class BioNeMoContainer:\n",
    "    \"\"\"Helper class for interacting with the BioNeMo container.\"\"\"\n",
    "    \n",
    "    def __init__(self, container_name=\"bionemo\"):\n",
    "        self.container_name = container_name\n",
    "    \n",
    "    def run(self, command):\n",
    "        \"\"\"Run a shell command.\"\"\"\n",
    "        return run_in_container(command)\n",
    "    \n",
    "    def python(self, code):\n",
    "        \"\"\"Run Python code.\"\"\"\n",
    "        return run_python_in_container(code)\n",
    "    \n",
    "    def script(self, path):\n",
    "        \"\"\"Run a Python script (use /workspace path for files in SageMaker dir).\"\"\"\n",
    "        return run_python_script_in_container(path)\n",
    "    \n",
    "    def gpu_memory(self):\n",
    "        \"\"\"Get GPU memory usage.\"\"\"\n",
    "        output = self.run(\"nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits\")\n",
    "        lines = output.strip().split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            used, total = line.split(', ')\n",
    "            print(f\"GPU {i}: {used}MB / {total}MB\")\n",
    "    \n",
    "    def list_files(self, path=\"/workspace\"):\n",
    "        \"\"\"List files in a directory.\"\"\"\n",
    "        return self.run(f\"ls -la {path}\")\n",
    "\n",
    "# Create instance\n",
    "bionemo = BioNeMoContainer()\n",
    "\n",
    "print(\"BioNeMoContainer helper created!\")\n",
    "print(\"\\nUsage:\")\n",
    "print('  bionemo.run(\"nvidia-smi\")')\n",
    "print('  bionemo.python(\"import torch; print(torch.cuda.is_available())\")')\n",
    "print('  bionemo.gpu_memory()')\n",
    "print('  bionemo.list_files(\"/workspace\")')\n",
    "print('  bionemo.script(\"/workspace/my_script.py\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: 0MB / 23028MB\n",
      "GPU 1: 0MB / 23028MB\n",
      "GPU 2: 0MB / 23028MB\n",
      "GPU 3: 0MB / 23028MB\n"
     ]
    }
   ],
   "source": [
    "bionemo.gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Workspace Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96\n",
      "drwxr-xr-x 10 ubuntu ubuntu  4096 Feb  4 02:17 .\n",
      "drwxr-xr-x  1 root   root      84 Feb  4 01:39 ..\n",
      "drwx------  4 ubuntu ubuntu  4096 Feb  4 01:04 .Trash-1000\n",
      "drwxrwxr-x  2 ubuntu ubuntu  4096 Feb  4 02:11 .ipynb_checkpoints\n",
      "drwxr-xr-x  2 ubuntu ubuntu  4096 Feb  4 00:15 .sparkmagic\n",
      "-rw-rw-r--  1 ubuntu ubuntu   517 Feb  4 02:17 .temp_script.py\n",
      "drwxrwxr-x  2 ubuntu ubuntu  4096 Feb  4 02:11 .virtual_documents\n",
      "-rw-rw-r--  1 ubuntu ubuntu  4398 Feb  4 01:37 README.md\n",
      "drwxr-xr-x  3 root   root    4096 Feb  4 01:40 bionemo2\n",
      "-rw-rw-r--  1 ubuntu ubuntu 17246 Feb  4 02:17 bionemo_interactive.ipynb\n",
      "drwxr-xr-x  3 root   root    4096 Feb  4 01:40 evo2_test\n",
      "-rw-rw-r--  1 ubuntu ubuntu  1856 Feb  4 01:58 launch_training.py\n",
      "drwx------  2 root   root   16384 Feb  4 00:15 lost+found\n",
      "drwxrwxr-x  3 ubuntu ubuntu  4096 Feb  4 01:38 src\n",
      "-rw-rw-r--  1 ubuntu ubuntu   253 Feb  4 02:13 temp_script.py\n",
      "-rw-rw-r--  1 ubuntu ubuntu  1196 Feb  4 01:37 test_cli.sh\n",
      "-rw-rw-r--  1 ubuntu ubuntu  1973 Feb  4 01:37 test_local.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bionemo.list_files(\"/workspace\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Custom Python Script\n",
    "\n",
    "Any Python file you save in `/home/ec2-user/SageMaker/` is accessible in the container at `/workspace/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script created. Running in container:\n",
      "Hello from BioNeMo container!\n",
      "GPUs available: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a script\n",
    "script = '''\n",
    "import logging\n",
    "logging.getLogger('nemo.utils.import_utils').setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "print(f\"Hello from BioNeMo container!\")\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "'''\n",
    "\n",
    "with open('/home/ec2-user/SageMaker/hello_bionemo.py', 'w') as f:\n",
    "    f.write(script)\n",
    "\n",
    "print(\"Script created. Running in container:\")\n",
    "print(bionemo.script(\"/workspace/hello_bionemo.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evo2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import of quick_gelu from megatron.core.fusions.fused_bias_geglu failed with: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/utils/import_utils.py\", line 319, in safe_import_from\n",
      "    return getattr(imported_module, symbol), True\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'megatron.core.fusions.fused_bias_geglu' has no attribute 'quick_gelu'\n",
      "\n",
      "INFO:nemo.utils.import_utils:Import of quick_gelu from megatron.core.fusions.fused_bias_geglu failed with: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/nemo/utils/import_utils.py\", line 319, in safe_import_from\n",
      "    return getattr(imported_module, symbol), True\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'megatron.core.fusions.fused_bias_geglu' has no attribute 'quick_gelu'\n",
      "\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Using byte-level tokenization\n",
      "[NeMo W 2026-02-04 02:19:42 nemo_logging:405] WandB is currently turned off.\n",
      "[NeMo W 2026-02-04 02:19:42 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Experiments will be logged at /workspace/evo2_notebook_test/notebook_test/dev\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393] Copying Trainer's 'max_steps' (5) to LR scheduler's 'max_steps'.\n",
      "[NeMo I 2026-02-04 02:19:42 num_microbatches_calculator:228] setting number of microbatches to constant 1\n",
      "[NeMo I 2026-02-04 02:19:42 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 812001024\n",
      "[NeMo I 2026-02-04 02:19:42 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)\n",
      "[NeMo I 2026-02-04 02:19:43 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "    Params for bucket 1 (812001024 elements, 812001024 padded size):\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.1.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.final_norm.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.embedding.word_embeddings.weight\n",
      "[NeMo I 2026-02-04 02:19:43 utils:661] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃   ┃ Name                                ┃ Type              ┃ Params ┃ Mode  ┃\n",
      "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│ 0 │ module                              │ DDP               │  812 M │ train │\n",
      "│ 1 │ module.module                       │ Float16Module     │  812 M │ train │\n",
      "│ 2 │ module.module.module                │ HyenaModel        │  812 M │ train │\n",
      "│ 3 │ module.module.module.embedding      │ LanguageModelEmb… │  2.1 M │ train │\n",
      "│ 4 │ module.module.module.rotary_pos_emb │ RotaryEmbedding   │      0 │ train │\n",
      "│ 5 │ module.module.module.decoder        │ HyenaStack        │  809 M │ train │\n",
      "│ 6 │ module.module.module.output_layer   │ ColumnParallelLi… │      0 │ train │\n",
      "└───┴─────────────────────────────────────┴───────────────────┴────────┴───────┘\n",
      "Trainable params: 812 M                                                         \n",
      "Non-trainable params: 0                                                         \n",
      "Total params: 812 M                                                             \n",
      "Total estimated model params size (MB): 3.2 K                                   \n",
      "Modules in train mode: 68                                                       \n",
      "Modules in eval mode: 0                                                         \n",
      "[NeMo W 2026-02-04 02:19:46 rerun_state_machine:1300] Implicit initialization of Rerun State Machine!\n",
      "[NeMo W 2026-02-04 02:19:46 rerun_state_machine:238] RerunStateMachine initialized in mode RerunMode.DISABLED\n",
      "Training epoch 0, iteration 0/4 | lr: 0 | global_batch_size: 1 | global_step: 0 | reduced_train_loss: 7.178 | train_step_timing in s: 3.623\n",
      "Training epoch 0, iteration 1/4 | lr: 1.2e-07 | global_batch_size: 1 | global_step: 1 | reduced_train_loss: 6.969 | train_step_timing in s: 1.154 | consumed_samples: 2\n",
      "Training epoch 0, iteration 2/4 | lr: 2.4e-07 | global_batch_size: 1 | global_step: 2 | reduced_train_loss: 7.115 | train_step_timing in s: 0.07386 | consumed_samples: 3\n",
      "Training epoch 0, iteration 3/4 | lr: 3.6e-07 | global_batch_size: 1 | global_step: 3 | reduced_train_loss: 7.143 | train_step_timing in s: 0.07297 | consumed_samples: 4\n",
      "Training epoch 0, iteration 4/4 | lr: 4.8e-07 | global_batch_size: 1 | global_step: 4 | reduced_train_loss: 7.267 | train_step_timing in s: 0.07261 | consumed_samples: 5\n",
      "[NeMo W 2026-02-04 02:19:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2026-02-04 02:19:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=5` reached.\n",
      "Validation: iteration 1/1\n"
     ]
    }
   ],
   "source": [
    "!docker exec bionemo train_evo2 \\\n",
    "     --mock-data \\\n",
    "     --model-size test \\\n",
    "     --num-nodes 1 \\\n",
    "     --devices 1 \\\n",
    "     --seq-length 128 \\\n",
    "     --micro-batch-size 1 \\\n",
    "     --global-batch-size 1 \\\n",
    "     --max-steps 5 \\\n",
    "     --result-dir /workspace/evo2_notebook_test \\\n",
    "     --experiment-name notebook_test \\\n",
    "     --disable-checkpointing \\\n",
    "     --limit-val-batches 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
